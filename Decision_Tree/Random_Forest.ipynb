{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Random Forest Algorithm </font>\n",
    "&nbsp; <a href = \"https://scholar.google.co.in/citations?user=pTdyt0YAAAAJ&hl=en\"> <font color='blue'> By Dr. Mohendra Roy (Ph. D.)</font> </a>\n",
    "\n",
    "&nbsp;<font color='blue'>Design Engineer at A. I. Medicines</font>&nbsp; &copy;  http://aimedicines.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we will impliment as well as explain the steps of Random Forest algorithm.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest algorithm is constituents of dicission trees. The voted output(the output the majority of trees provide) for a perticular test row is selected from the collection of trees from the forest. There fore in this algorithm we have to make the following functions:\n",
    "\n",
    "1) Random Forest consisting of decision trees\n",
    "\n",
    "2) Functions for dicision tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Random Forest function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Random_Forest(training_data, test_data, maximum_branch, minimum_sample, subsample_ratio, tree_number, no_of_features):\n",
    "\ttrees = list()\n",
    "\tfor i in range(tree_number):\n",
    "\t\tsub_sample = Make_Subsample(training_data, subsample_ratio) # \"Make_Subsample\" is a user defined function \n",
    "\t\ttree = Make_Tree(sub_sample, maximum_branch, minimum_sample, no_of_features)#\"Make_Tree\" is a user defined function\n",
    "\t\ttrees.append(tree)\n",
    "\toutput = [Voted_Output(trees, row) for row in test_data]\n",
    "\treturn(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here **\"training_data\"** argument is the dataset for training the **<font color='red'>\"Random_Forest\"</font>** function. Subsiquently **\"test_data\"** is the dataset to test the performance of the algorithm. \n",
    "\n",
    "**maximum_branch** is the agrument for the maximum _allowed branch_ or _depth_ in a tree. Similarly ** minimum_sample ** is the minimum number of sample required in a **_node_** for further brancing from that node. \n",
    "\n",
    "\n",
    "**subsample_ratio** is the ratio at which the training sample will be randomly devided to subsamples for the training of the trees.\n",
    "\n",
    "**tree_number** is the total number of trees that preset in the random forest\n",
    "\n",
    "**no_of_features** is the total number of random features that will use by a tree for the training. Each tree will use different sets of features for training. and the minimum number of features required for the traing of a tree is given by ---  \n",
    "\n",
    "$$ Number \\ of \\ Features = \\sqrt{Length \\ of\\ a\\ row\\ in\\  dataset } $$\n",
    "\n",
    "**<font color = 'red'> Note: </font>** Here we use the following user defined functions :\n",
    "\n",
    "\n",
    "** <font color = 'blue'> 1) Make_Subsample,    2)  Make_Tree,  3) Voted_Output </font> **\n",
    "\n",
    "Further we will explain these functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functin for making subsample from the training sample. \n",
    "\n",
    "from random import randrange # \"randrange\" generates random number for a given range\n",
    "\n",
    "def Make_Subsample(dataset, subsample_ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataset) * subsample_ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataset))\n",
    "\t\tsample.append(dataset[index])\n",
    "\treturn sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will return a random subset of the given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for making tree\n",
    "\n",
    "def Make_Tree(training_data, maximum_branch, minimum_sample, no_of_features):\n",
    "\troot = Make_Label(training_data, no_of_features) # \"Make_Label\" is a user defined function \n",
    "\tChild_Split(root, maximum_branch, minimum_sample, no_of_features, 1) # Child_Split is a user defined function\n",
    "\treturn root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will make tree. In this function we used the <font color = blue> **Make_Label** </font> and <font color = blue> **Child_Split**</font> functions. We will discuss about these after the **<font color = blue> Voted_Output</font>** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for calculating voted output \n",
    "def Voted_Output(trees, test_row):\n",
    "\tp = [Predict(tree, test_row) for tree in trees] # \"Predict\" is a user defined function\n",
    "\treturn max(set(p), key=p.count) # This will return the maximum occured number in the row in **\"p\"** provided by the\n",
    "# Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, the **\"trees\"** argument is retured by the  **<font color = 'blue'> Make_Tree </font> ** function. Again in the above function we used the **<font color = 'blue'> Predict </font> ** function. The detail of the function is as given bellow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to predict the test row to be in a perticular group\n",
    "\n",
    "def Predict(node, test_row):\n",
    "\tif test_row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict): \n",
    "\t\t\treturn Predict(node['left'], test_row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn Predict(node['right'], test_row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the argument **\"node\"** is returned by the **<font color = 'blue'> Make_Tree </font>** function. Which contain the dictionary containing the **\"index\"**, **\"value\"**, **label**( or label_value) of the termination node and if applicable, sub groups of all of these for sub branch.\n",
    "\n",
    "Note: ***\"isinstance\"*** function  return, whether an object is a instance of a class or of a subclass thereof. Here if the left(or right) itsef contain the dictionaly(**dict**) then it will again operate the \"Predict\" function on the left(or right)\n",
    "itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make label\n",
    "\n",
    "def Make_Label(dataset, no_of_features):\n",
    "\tclass_labels = list(set(row[-1] for row in dataset))\n",
    "\tbindex, bvalue, bscore, bgroups = 1000, 1000, 1000, None # Innitial labels(with values) of the dictionary \n",
    "\tfeatures = list()\n",
    "\twhile len(features) < no_of_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = Data_Split(index, row[index], dataset) # \"Data_Split\" is a user defined function\n",
    "\t\t\tgini = Gini_Index(groups, class_labels) # \"Gini Index\" is a user defined function \n",
    "\t\t\tif gini < bscore:\n",
    "\t\t\t\tbindex, bvalue, bscore, bgroups = index, row[index], gini, groups\n",
    "\treturn {'index':bindex, 'value':bvalue, 'groups':bgroups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the ***\"class_labels\"*** contains the labels or the label numbers, each from a perticular Row of the dataset(train set). \n",
    "\n",
    "Here we used the **<font color = 'blue'> Data_Split</font>** and **<font color = 'blue'> Gini_Index </font>** build in functions. These are as explained in bellow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to split the data based on the index and value that have supplied to it.\n",
    "\n",
    "def Data_Split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for making the sub nodes by spliting the parent node to obtain branches of the tree and eventually end node.\n",
    "\n",
    "def Child_Split(node,maximum_branch, minimum_sample, no_of_features, current_depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = Terminate(left + right) # \"Terminate\" is a user defined function\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif current_depth >= maximum_branch:\n",
    "\t\tnode['left'], node['right'] = Terminate(left), Terminate(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= minimum_sample:\n",
    "\t\tnode['left'] = Terminate(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = Make_Label(left, n_features)\n",
    "\t\tChild_Split(node['left'],maximum_branch, minimum_sample, no_of_features, current_depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = Terminate(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = Make_Label(right, n_features)\n",
    "\t\tChild_Split(node['right'], maximum_branch, minimum_sample, no_of_features, current_depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used a user defined functin called **<font color = blue> Terminate </font>**. Bellow we will frame this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Terminate(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***\"return max(set(outcomes), key=outcomes.count)\"*** command gives the output of the of the maximum occured number in the **outcomes** array.\n",
    "\n",
    "We know that Gini Index is one of the cost function that is used in the decision tree algorithm. And this is given by:$$ Gini(f) = \\sum_{i=1}^{J} c_i (1-c_i) $$ Where c is the appearance of the class in a perticular group and i = 1,...J are class types ( or class labels). This is for a perticular one group. For actual Gini Index we have to take the weighted sum. $$ Gini\\ Index(f) = \\sum_{g=1}^{n}  \\sum_{i=1}^{J} \\frac{1}{Length(G_n)} c_i (1-c_i) . $$ Where G is the group and g =1,..n is the number of groups. \n",
    "\n",
    "\n",
    "We can impliment this as a function as described in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to determine the Gini Index\n",
    "def Gini_Index(groups, class_labels):\n",
    "\tgini = 0.0\n",
    "\tfor class_value in class_labels:\n",
    "\t\tfor group in groups:\n",
    "\t\t\tsize = len(group)\n",
    "\t\t\tif size == 0:\n",
    "\t\t\t\tcontinue # This is to avoid the empty groups\n",
    "\t\t\tproportion = [row[-1] for row in group].count(class_value) / float(size)\n",
    "\t\t\tgini += (proportion * (1.0 - proportion))\n",
    "\treturn gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Till now we have build the Random forest algorithm. Now we will write function to evaluate the performance of the Random forest algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the Random Forest algorithm \n",
    "\n",
    "def Evaluate_Algorithm(dataset, algorithm, number_of_folds, *args): # *args contain list of arguments\n",
    "\tfolds = Make_Fold(dataset, number_of_folds) # \"Make_Fold\" is a user define function\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold) # this is because we will use the current fold as a test set\n",
    "\t\ttrain_set = sum(train_set, []) # this will make the train set containing purely one group( only one matrix)\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None # to remove the label from the test set\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args) # Further we will replace the \"algorithm\" with \"Random_Forest\"\n",
    "\t\tactual = [row[-1] for row in fold] # copy the labels of the test set\n",
    "\t\taccuracy = Accuracy(actual, predicted) # \"Accuracy\" is the user defined function\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the ** <font color=blue> Make_Fold </font>** and ** <font color=blue> Accuracy </font>** user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  function to Make Fold of the database to get the training and testing sub dataset \n",
    "\n",
    "def Make_Fold(dataset, number_of_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / number_of_folds)\n",
    "\tfor i in range(number_of_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to find the accuracy betwwen the predicted and actual data\n",
    "\n",
    "def Accuracy(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make functions to load the file and make ready the dataset for apply in Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "** N.B.** Here I intetinaly write the user defined functions starting with capital letter to differentiate between the build in functions and user defined functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "\n",
    "\n",
    "**Acknowledgement:** I deeply thankful to <a href = \"http://machinelearningmastery.com/about/\"> <font color='blue'> Jason Brownlee </font> </a>\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:** \n",
    "\n",
    "[1] http://machinelearningmastery.com/blog/\n",
    "\n",
    "[2] Machine Learning: A Probabilistic Perspective. A textbook by Kevin R Murphy. ISBN: 978-0-262-01802-9\n",
    "\n",
    "[3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
