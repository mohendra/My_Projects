{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "#from sklearn.datasets import fetch_mldata # This is to import MNIST dataset\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layer(Input, Output_Nodes):\n",
    "    [r,c] = np.shape(Input)\n",
    "    limit = 1 / math.sqrt(10)\n",
    "            #self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
    "        #self.w0 = np.zeros((1, self.n_units))\n",
    "        \n",
    "    Weights = np.random.uniform(-limit,limit,(c, Output_Nodes ))\n",
    "    Bias = np.zeros((1, Output_Nodes))\n",
    "    Layer = [Weights,Bias]\n",
    "    \n",
    "    return Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Layers(Input_Layer, New_Layer_Size):\n",
    "    Layers_Together = []\n",
    "    Input_Layer_weight = Input_Layer[0]\n",
    "    New_Layer = Layer(Input_Layer_weight, New_Layer_Size)\n",
    "    \n",
    "    Layers_Together.append(Input_Layer)\n",
    "    Layers_Together.append(New_Layer)\n",
    "    \n",
    "    return Layers_Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leaky_ReLu(x):\n",
    "    return np.where(x >= 0, x, x*0.2)\n",
    "\n",
    "def Leaky_ReLu_Prime(x):\n",
    "    return np.where(x >= 0, 1, 0.2)\n",
    "\n",
    "def TanH(x):\n",
    "    return 2 / (1 + np.exp(-2*x)) - 1\n",
    "def TanH_Prime(x):\n",
    "    return 1 - np.power(TanH(x), 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sqr_loss(y, y_pred):\n",
    "    return 0.5 * np.power((y - y_pred), 2)\n",
    "\n",
    "def Sqr_loss_Prime(y, y_pred): #Gradient of Loss function\n",
    "    return -(y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update(w, grad): # Update with Adam Optimizer\n",
    "    b1=0.9\n",
    "    b2=0.999\n",
    "    LR=0.000010\n",
    "    eps =1e-8\n",
    "    m=np.zeros(np.shape(grad))\n",
    "    v=np.zeros(np.shape(grad))\n",
    "    mm = b1*m + (1 - b1)*grad\n",
    "    vv = b2*v + (1 - b2)*np.power(grad, 2)\n",
    "    m_hat = mm / (1 - b1)\n",
    "    v_hat = vv / (1 - b2)\n",
    "    w_updt = LR * m_hat / (np.sqrt(v_hat) + eps)\n",
    "    return w-w_updt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(Input,Para_Layer,Activationt_type): # the function for froward pass\n",
    "    H = np.dot(Input,Para_Layer[0])+Para_Layer[1]\n",
    "    \n",
    "    if Activationt_type==1:\n",
    "        A = Leaky_ReLu(H)\n",
    "        Ad = Leaky_ReLu_Prime(H)\n",
    "    else:\n",
    "        A = TanH(H)\n",
    "        Ad = TanH_Prime(H)\n",
    "        \n",
    "    \n",
    "    HL = [A, Ad] # Here Ad is the gradient of activation\n",
    "    return HL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropa_of_Activated(Gradient,Gradient_activation): # this is the function for calculating gradient of the activation layer\n",
    "    return Gradient*Gradient_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropa_dense(Gradient,Previus_activation_layer, Current_Layer): # function for calculating barckprogation of dense with Logits layera\n",
    "    grad_W = Previus_activation_layer.T.dot(Gradient) # Calculating gradient with respect to W\n",
    "    grad_bias = np.sum(Gradient,axis=0, keepdims=True) # calculating gradient with respect to bias\n",
    "    W_Update = Update(Current_Layer[0], grad_W) # Update the Weights\n",
    "    Bias_Update = Update(Current_Layer[1], grad_bias) # Update the bias\n",
    "    new_Layer = [W_Update,Bias_Update] # The UPdated Weight and baises\n",
    "    \n",
    "    New_Gradient = Gradient.dot(W_Update.T) # The new gradient with the Updated W\n",
    "    return New_Gradient, new_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = range(1,10)\n",
    "imgs =np.array(input).reshape(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the neural network\n",
    "L1 = Layer(imgs, 5)\n",
    "L2 = Layer(L1[0],2)\n",
    "L3 = Layer(L2[0],5)\n",
    "L4 = Layer(L3[0],np.shape(imgs)[1])\n",
    "Layers = [L1,L2,L3,L4]\n",
    "#Layer_Size = np.size(Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Input is [1 2 3 4 5 6 7 8 9]\n",
      "The Output is [1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = 100\n",
    "Loss_store = []\n",
    "#epoch = 1111\n",
    "while loss>0.001:\n",
    "#for epo in range(epoch):\n",
    "    #idx = np.random.randint(0, X.shape[0], 100) # here 10 is the batch size\n",
    "    #imgs = X[idx]\n",
    "    i=0\n",
    "    keep_Latent =[]\n",
    "    for L in Layers:\n",
    "        i = i+1\n",
    "        #print('Opering Layer = ',i)\n",
    "\n",
    "        if i <=1: # if it is the first Layer\n",
    "            Latent_Layer = forward(imgs,L,1)\n",
    "        else:\n",
    "            Latent_Layer = forward(Latent_Layer[0],L,1)\n",
    "\n",
    "        keep_Latent.append(Latent_Layer)\n",
    "\n",
    "    Last_Latent = keep_Latent[-1] # THe Keep_Latent contain both latent and ativation layer\n",
    "    output = Last_Latent[0]  # This is the latent layer\n",
    "    loss = np.mean(Sqr_loss(imgs, output)) \n",
    "    Loss_store.append(loss)\n",
    "    #print (\"%d [D loss: %f]\" % (epoch, loss))\n",
    "\n",
    "    Gradient = Sqr_loss_Prime(imgs, output)\n",
    "    gradient_1 = Gradient\n",
    "    copy_keep_Latent = copy.copy(keep_Latent)\n",
    "    copy_keep_Latent.reverse()\n",
    "\n",
    "    g=0\n",
    "    Updated_Layer = []\n",
    "    for R in reversed(Layers):\n",
    "        g=g+1 \n",
    "        #print(g)\n",
    "        Current_reversed_latent_Layer = copy_keep_Latent[g-1]\n",
    "        \n",
    "        Current_AG = Current_reversed_latent_Layer[1] # current Gradient activation fucntion\n",
    "\n",
    "        if g<=1: # if this is the last layer of the network (since the layer is reversed here)\n",
    "            Gradient = backpropa_of_Activated(Gradient,Current_AG)\n",
    "            gradient_2 = Gradient \n",
    "            #Gradient=Gradient+copy_gradient\n",
    "        else:\n",
    "            Gradient = backpropa_of_Activated(Gradient,Current_AG)\n",
    "            gradient_3 = Gradient\n",
    "\n",
    "        Current_Layer=R # Current weight and bias\n",
    "\n",
    "\n",
    "        if g<=3: # Here 3 is because their are only 4 layers in the netowrk and the last layer here is input layer\n",
    "            Previus_Latent_layer = copy_keep_Latent[g]\n",
    "            Previus_activation_layer = Previus_Latent_layer[0]\n",
    "\n",
    "            Gradient, new_Layer=backpropa_dense(Gradient,Previus_activation_layer, Current_Layer)\n",
    "            gradient_4 = Gradient\n",
    "        else: # if the layer is the input layer\n",
    "            Gradient, new_Layer=backpropa_dense(Gradient,imgs, Current_Layer)\n",
    "            gradient_5 = Gradient\n",
    "\n",
    "\n",
    "\n",
    "        Updated_Layer.append(new_Layer)\n",
    "\n",
    "        new_Weight_and_bias = copy.copy(Updated_Layer)\n",
    "        new_Weight_and_bias.reverse()\n",
    "\n",
    "        Layers = new_Weight_and_bias\n",
    "\n",
    "    #print(loss)\n",
    "output_array = np.round(Last_Latent[0][0])    \n",
    "print('The Input is',imgs[0])\n",
    "print('The Output is',output_array)\n",
    "plt.plot(Loss_store)\n",
    "plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009987377873459464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
