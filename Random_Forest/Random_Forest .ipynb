{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Random Forest Algorithm </font>\n",
    "&nbsp; <a href = \"https://scholar.google.co.in/citations?user=pTdyt0YAAAAJ&hl=en\"> <font color='blue'> By Dr. Mohendra Roy (Ph. D.)</font> </a>\n",
    "\n",
    "&nbsp;<font color='blue'>Design Engineer at A. I. Medicines</font>&nbsp; &copy;  http://aimedicines.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Wikipedia, a forest is a large area dominated by **trees** [1]. In machine learning, the same analogy has been used for building **ensemble algorithms** such as Random Forest. Ensemble technique contains group of predictive models (such as decision trees) to achieve a better accuracy and stability by avoiding bias and higher variance. Here Bias is, 'how much on an average are the predicted values different from the actual value' and variance means, 'how different will the predictions of the model be at the same point if different samples are taken from the same population' [2].\n",
    "\n",
    "\n",
    "Generally, we can increase the prediction accuracy by increasing the complexity of the decision tree (by increasing the branches or sub nodes), which will lower the bias in the model. However, this will **over fit** the model, which intern will suffer from high variance. Therefore, we must find a better way to make a balance between these two errors. \n",
    "\n",
    "In these regards the random forest comes into account. The random forest is based on **bagging technique** (Bootstrap aggregating) [3]. This reduces the variance in the predictions by combining the result of multiple classifiers (trees) modeled on different sub-samples of the same dataset. In short the random forest algorithm provides an output based on the most predicted value by the maximum number of trees in that forest.\n",
    "\n",
    "In this chapter we will discuss and develope the Random Forest algorithm from scratch. \n",
    "\n",
    "The concise steps of the algorithm is as follows..\n",
    "\n",
    "1) Fold the dataset to N number of sub dataset. These N number of dataset(with replacement) will then be used for the training of random forest model for n number of trees ( with minimum features = sqrt(length of features in the dataset). \n",
    "\n",
    "\n",
    "2) Grow the each tree to the largest extent possible using a cost function ( here we will use Gini Index).\n",
    "\n",
    "3) Find the maximum predicted output by the trees (for classification problem) or average of the predicted output (for regression problem)\n",
    "\n",
    "For the training and testing of this algorithm we will use a sonar dataset. The sonar dataset contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. It also contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock. \n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp. \n",
    "\n",
    "The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.\n",
    "\n",
    "We will design our model on the top of this dataset to differentiate between the rock and metal as well as test it using the test sample from the same dataset. Here we go ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Random Forest function: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Random_Forest(training_data, test_data, maximum_branch, minimum_sample, subsample_ratio, tree_number, no_of_features):\n",
    "\ttrees = list()\n",
    "\tfor i in range(tree_number):\n",
    "\t\tsub_sample = Make_Subsample(training_data, subsample_ratio) # \"Make_Subsample\" is a user defined function \n",
    "\t\ttree = Make_Tree(sub_sample, maximum_branch, minimum_sample, no_of_features)#\"Make_Tree\" is a user defined function\n",
    "\t\ttrees.append(tree)\n",
    "\toutput = [Voted_Output(trees, row) for row in test_data]\n",
    "\treturn(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here **\"training_data\"** argument is the dataset for training the **<font color='red'>\"Random_Forest\"</font>** function. Subsiquently **\"test_data\"** is the dataset to test the performance of the algorithm. \n",
    "\n",
    "**maximum_branch** is the agrument for the maximum _allowed branch_ or _depth_ in a tree. Similarly ** minimum_sample ** is the minimum number of sample required in a **_node_** for further brancing from that node. \n",
    "\n",
    "\n",
    "**subsample_ratio** is the ratio at which the training sample will be randomly devided to subsamples for the training of the trees.\n",
    "\n",
    "**tree_number** is the total number of trees that preset in the random forest\n",
    "\n",
    "**no_of_features** is the total number of random features that will use by a tree for the training. Each tree will use different sets of features for training. and the minimum number of features required for the traing of a tree is given by [4] ---  \n",
    "\n",
    "$$ Number \\ of \\ Features = \\sqrt{Length \\ of\\ a\\ row\\ in\\  dataset } $$\n",
    "\n",
    "**<font color = 'red'> Note: </font>** Here we use the following user defined functions :\n",
    "\n",
    "\n",
    "** <font color = 'blue'> 1) Make_Subsample,    2)  Make_Tree,  3) Voted_Output </font> **\n",
    "\n",
    "Further we will explain these functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functin for making subsample from the training sample. \n",
    "\n",
    "from random import randrange # \"randrange\" generates random number for a given range\n",
    "\n",
    "def Make_Subsample(dataset, subsample_ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataset) * subsample_ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataset))\n",
    "\t\tsample.append(dataset[index])\n",
    "\treturn sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will return a random subset of the given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for making tree\n",
    "\n",
    "def Make_Tree(training_data, maximum_branch, minimum_sample, no_of_features):\n",
    "\troot = Make_Label(training_data, no_of_features) # \"Make_Label\" is a user defined function \n",
    "\tChild_Split(root, maximum_branch, minimum_sample, no_of_features, 1) # Child_Split is a user defined function\n",
    "\treturn root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will make tree. In this function we used the **<font color = blue>Make_Label</font>** and **<font color = blue>Child_Split</font>** functions. We will discuss about these after the **<font color = blue> Voted_Output</font>** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for calculating voted output \n",
    "def Voted_Output(trees, test_row):\n",
    "\tp = [Predict(tree, test_row) for tree in trees] # \"Predict\" is a user defined function\n",
    "\treturn max(set(p), key=p.count) # This will return the maximum occured number in the row in **\"p\"** provided by the\n",
    "# Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, the **\"trees\"** argument is retured by the  **<font color = 'blue'> Make_Tree </font> ** function. Again in the above function we used the **<font color = 'blue'> Predict </font> ** user defined function. The detail of the function is as given bellow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to predict the test row to be in a perticular group\n",
    "\n",
    "def Predict(node, test_row):\n",
    "\tif test_row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict): \n",
    "\t\t\treturn Predict(node['left'], test_row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn Predict(node['right'], test_row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the argument **\"node\"** is returned by the **<font color = 'blue'> Make_Tree </font>** function. Which contain the dictionary containing the **\"index\"**, **\"value\"**, **label**( or label_value) of the termination node and if applicable, sub groups of all of these for sub branch.\n",
    "\n",
    "Note: ***\"isinstance\"*** function  return, whether an object is a instance of a class or of a subclass thereof. Here if the left(or right) itsef contain the dictionaly(**dict**) then it will again operate the \"Predict\" function on the left(or right)\n",
    "itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make label\n",
    "\n",
    "def Make_Label(dataset, no_of_features):\n",
    "\tclass_labels = list(set(row[-1] for row in dataset))\n",
    "\tbindex, bvalue, bscore, bgroups = 1000, 1000, 1000, None # Innitial labels(with values) of the dictionary \n",
    "\tfeatures = list()\n",
    "\twhile len(features) < no_of_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = Data_Split(index, row[index], dataset) # \"Data_Split\" is a user defined function\n",
    "\t\t\tgini = Gini_Index(groups, class_labels) # \"Gini Index\" is a user defined function \n",
    "\t\t\tif gini < bscore:\n",
    "\t\t\t\tbindex, bvalue, bscore, bgroups = index, row[index], gini, groups\n",
    "\treturn {'index':bindex, 'value':bvalue, 'groups':bgroups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the ***\"class_labels\"*** contains the labels or the label numbers, each from a perticular Row of the dataset(train set). \n",
    "\n",
    "Here we used the **<font color = 'blue'> Data_Split</font>** and **<font color = 'blue'> Gini_Index </font>** build in functions. These are as explained in bellow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to split the data based on the index and value that have supplied to it.\n",
    "\n",
    "def Data_Split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for making the sub nodes by spliting the parent node to obtain branches of the tree and eventually end node.\n",
    "\n",
    "def Child_Split(node,maximum_branch, minimum_sample, no_of_features, current_depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = Terminate(left + right) # \"Terminate\" is a user defined function\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif current_depth >= maximum_branch:\n",
    "\t\tnode['left'], node['right'] = Terminate(left), Terminate(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= minimum_sample:\n",
    "\t\tnode['left'] = Terminate(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = Make_Label(left, no_of_features)\n",
    "\t\tChild_Split(node['left'],maximum_branch, minimum_sample, no_of_features, current_depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= minimum_sample:\n",
    "\t\tnode['right'] = Terminate(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = Make_Label(right, no_of_features)\n",
    "\t\tChild_Split(node['right'], maximum_branch, minimum_sample, no_of_features, current_depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used a user defined functin called **<font color = blue> Terminate </font>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Terminate(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***\"return max(set(outcomes), key=outcomes.count)\"*** command gives the output of the of the maximum occured number in the **outcomes** array.\n",
    "\n",
    "We know that Gini Index is one of the cost function that is used in the decision tree algorithm. And this is given by:$$ Gini\\ index =\\sum_{c=1}^{m} \\sum_{i=1}^{n} Py_i (1-Py_i) $$ Where Py is the probability of the of yes for a perticular class type in group _i_ ( and \"_i_\" is ranging from 1 to n number of groups) and _c_ is the class ranging from 1 to m. We can impliment this as a function as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to determine the Gini Index\n",
    "def Gini_Index(groups, class_labels):\n",
    "\tgini = 0.0\n",
    "\tfor class_value in class_labels:\n",
    "\t\tfor group in groups:\n",
    "\t\t\tsize = len(group)\n",
    "\t\t\tif size == 0:\n",
    "\t\t\t\tcontinue # This is to avoid the empty groups\n",
    "\t\t\tprob = [row[-1] for row in group].count(class_value) / float(size)\n",
    "\t\t\tgini += (prob * (1.0 - prob))\n",
    "\treturn gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Till now we have build the Random forest algorithm. Now we will write function to evaluate the performance of the Random forest algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the Random Forest algorithm \n",
    "\n",
    "def Evaluate_Algorithm(dataset, algorithm, number_of_folds, *args): # *args contain list of arguments\n",
    "\tfolds = Make_Fold(dataset, number_of_folds) # \"Make_Fold\" is a user define function\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold) # this is because we will use the current fold as a test set\n",
    "\t\ttrain_set = sum(train_set, []) # this will make the train set containing purely one group( only one matrix)\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None # to remove the label from the test set\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args) # Further we will replace the \"algorithm\" with \"Random_Forest\"\n",
    "\t\tactual = [row[-1] for row in fold] # copy the labels of the test set\n",
    "\t\taccuracy = Accuracy(actual, predicted) # \"Accuracy\" is the user defined function\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the ** <font color=blue> Make_Fold </font>** and ** <font color=blue> Accuracy </font>** user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  function to Make Fold of the database to get the training and testing sub dataset \n",
    "\n",
    "def Make_Fold(dataset, number_of_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / number_of_folds)\n",
    "\tfor i in range(number_of_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to find the accuracy betwwen the predicted and actual data\n",
    "\n",
    "def Accuracy(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make functions to **load** the file and make ready the dataset for apply in Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for importing CSV file.\n",
    "from csv import reader # reader will read the file\n",
    "\n",
    "# Load a CSV file\n",
    "def Load_File(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file: # open the file in read only mode\n",
    "\t\tdata = reader(file)\n",
    "\t\tfor row in data:\n",
    "\t\t\tif not row: # this is to eleminate any empty row\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will prepare the dataset. That is, to convert the **label of the dataset from <font color = red> string to integer</font>** as well as convert the datas that are preset as a string (in csv file) to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert string column to float\n",
    "def Str_Float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert labels to integer\n",
    "def Label_Int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tlabels= set(class_values)\n",
    "\tL = dict()\n",
    "\tfor i, label in enumerate(labels):\n",
    "\t\tL[label] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = L[row[column]]\n",
    "\treturn L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test:** Now we will test our algorithm. We have dowonloded the sd.csv (Sonar Dataset Case Study) file from <a href = \"https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\"> <font color='blue'> UC Irvine Machine Learning Repository </font> </a>\n",
    "\n",
    "**About the dataset:**\n",
    "\n",
    "*NAME*: Sonar, Mines vs. Rocks\n",
    "\n",
    "*SUMMARY*: This is the data set used by Gorman and Sejnowski in their study\n",
    "of the classification of sonar signals using a neural network.  The\n",
    "task is to train a network to discriminate between sonar signals bounced\n",
    "off a metal cylinder and those bounced off a roughly cylindrical rock.\n",
    "\n",
    "*SOURCE*: The data set was contributed to the benchmark collection by Terry\n",
    "Sejnowski, now at the Salk Institute and the University of California at\n",
    "San Deigo.  The data set was developed in collaboration with R. Paul\n",
    "Gorman of Allied-Signal Aerospace Technology Center.\n",
    "\n",
    "We used the custom developed Load_file function to load the file and subsiquently convert the datas to float and labels to integer data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [65.85365853658537, 68.29268292682927, 56.09756097560976, 56.09756097560976, 53.65853658536586]\n",
      "Mean Accuracy: 60.000%\n",
      "Trees: 5\n",
      "Scores: [63.41463414634146, 53.65853658536586, 53.65853658536586, 58.536585365853654, 75.60975609756098]\n",
      "Mean Accuracy: 60.976%\n",
      "Trees: 10\n",
      "Scores: [65.85365853658537, 70.73170731707317, 63.41463414634146, 58.536585365853654, 63.41463414634146]\n",
      "Mean Accuracy: 64.390%\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt # it will require to find the minimum number of features.\n",
    "\n",
    "# load and prepare data\n",
    "filename = '/Users/mohendra/Dropbox/My_Projects/Test/sd.csv'\n",
    "dataset = Load_File(filename)\n",
    "\n",
    "# convert string attributes to integers\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "\tStr_Float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "Label_Int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "\n",
    "number_of_folds = 5\n",
    "maximum_branch = 10\n",
    "minimum_sample = 1\n",
    "subsample_ratio = 1.0\n",
    "\n",
    "no_of_features = int(sqrt(len(dataset[0])-1))\n",
    "\n",
    "for tree_number in [1, 5, 10]:\n",
    "\tscores = Evaluate_Algorithm(dataset, Random_Forest, number_of_folds, maximum_branch, minimum_sample, subsample_ratio, tree_number, no_of_features)\n",
    "\tprint('Trees: %d' % tree_number)\n",
    "\tprint('Scores: %s' % scores)\n",
    "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Random Forest:**\n",
    "\n",
    "1) This algorithm can solve both classification and regression problems.\n",
    "\n",
    "2) It can handle large data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "** N.B.** Here we used capital letter in user defined functions to differentiate between the build in functions and user defined functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "\n",
    "\n",
    "**Acknowledgement:** I deeply thankful to Jason Brownlee \n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:** \n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Forest\n",
    "\n",
    "[2] http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "[4] http://machinelearningmastery.com/blog/\n",
    "\n",
    "[5] Machine Learning: A Probabilistic Perspective. A textbook by Kevin R Murphy. ISBN: 978-0-262-01802-9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
